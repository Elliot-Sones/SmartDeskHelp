# Conversational Training Guide

## Answer to Your Question: "Are we only adding conversation to phase 2-4?"

**No.** Here's the correct understanding:

### Original 4-Phase Plan (Mixed Approach - NOT RECOMMENDED)
```
Phase 1: Extractive foundation ‚ùå NOT conversational
Phase 2: Extractive reasoning   ‚ùå NOT conversational
Phase 3: Conversational         ‚úÖ CONVERSATIONAL (NEW)
Phase 4: Extractive refusals    ‚ùå NOT conversational
```

**Problem**: Mixing extractive and conversational is inconsistent. Model gets confused about output style.

---

## RECOMMENDED: 3-Phase Consistent Conversational Approach

### The Right Way

```
Phase 1: Extractive Foundation (37K examples)
  ‚îú‚îÄ Purpose: Teach model to FIND answers in context
  ‚îú‚îÄ Data: generated/layer1_squad.jsonl
  ‚îî‚îÄ Output: "Paris", "RAM at 97%", "Documents/file.pdf"

Phase 2: Conversational Answers (75K examples)
  ‚îú‚îÄ Purpose: Teach model to SAY answers naturally
  ‚îú‚îÄ Data: generated/train_conversational_positive.jsonl (NEW - generated by Haiku)
  ‚îî‚îÄ Output: "The capital is Paris.", "Your RAM is at 97%, causing slowdowns."

Phase 3: Conversational Refusals (38K examples)
  ‚îú‚îÄ Purpose: Teach model to REFUSE politely
  ‚îú‚îÄ Data: generated/train_conversational_negative.jsonl (NEW - generated by Haiku)
  ‚îî‚îÄ Output: "I couldn't find that information in the provided context."
```

**Total**: 150K examples across 3 phases

---

## Why This Order?

### Phase 1: Extractive Foundation (MUST BE FIRST)
- **Purpose**: Teach the model WHERE to look in the context
- **Critical**: If you skip this, the model will hallucinate nice-sounding answers that aren't from the context
- **Analogy**: Teaching a student to highlight the answer before teaching them to explain it

### Phase 2: Conversational Positive (AFTER foundation)
- **Purpose**: Now that it knows HOW to find answers, teach it HOW to phrase them
- **Data source**: Take Phase 1's extractive answers ‚Üí Send to Claude Haiku ‚Üí Get conversational versions
- **Result**: Model can find AND articulate answers naturally

### Phase 3: Conversational Refusals (LAST)
- **Purpose**: Teach polite refusals when answer not found
- **Data source**: Take negative examples ‚Üí Send to Claude Haiku ‚Üí Get varied polite refusals
- **Result**: Model refuses gracefully instead of hallucinating

---

## What Gets Generated by Claude Haiku?

### For Positive Examples (Phase 2)

**Before (extractive):**
```json
{"input": "Context: RAM usage 7.8GB/8GB (97%)\nQuestion: Why slow?", "output": "RAM usage 7.8GB/8GB (97%)"}
```

**Sent to Haiku:**
```
System: You are a helpful AI assistant...
User: Rewrite "RAM usage 7.8GB/8GB (97%)" as a conversational answer to "Why slow?"
```

**After (conversational):**
```json
{"input": "Context: RAM usage 7.8GB/8GB (97%)\nQuestion: Why slow?", "output": "Your computer is slow because RAM is at 97% capacity (7.8GB out of 8GB). Try closing some applications."}
```

### For Negative Examples (Phase 3)

**Before (extractive):**
```json
{"input": "Context: random text\nQuestion: Who is my boss?", "output": "Not found in provided context."}
```

**Sent to Haiku:**
```
System: You are a helpful AI assistant that provides polite refusals...
User: Rephrase "Not found in provided context." for question "Who is my boss?"
```

**After (conversational):**
```json
{"input": "Context: random text\nQuestion: Who is my boss?", "output": "I couldn't find information about your boss in the provided context."}
```

---

## Update 02_train.py to Use This Structure

```python
PHASES = [
    # Phase 1: Learn to extract (foundation)
    ("üü¢ PHASE 1: EXTRACTION", "generated/layer1_squad.jsonl", "Extractive foundation"),
    # ~37K examples
    # Output: "Paris", "RAM at 97%", "docs/file.pdf"

    # Phase 2: Learn to be conversational (positive answers)
    ("üîµ PHASE 2: CONVERSATIONAL", "generated/train_conversational_positive.jsonl", "Natural responses"),
    # ~75K examples (50% of dataset)
    # Output: "The capital is Paris.", "Your RAM is at 97%..."

    # Phase 3: Learn polite refusals
    ("üü† PHASE 3: REFUSALS", "generated/train_conversational_negative.jsonl", "Polite refusals"),
    # ~38K examples (25% of dataset)
    # Output: "I couldn't find that information..."
]
```

---

## Exact Workflow

### Step 1: Generate Conversational Data

```bash
# Set API key
export ANTHROPIC_API_KEY='sk-ant-...'

# Generate BOTH positive and negative conversational data
cd T5finetuning
python 01_prepare_data_conversational.py
```

**What happens:**
1. Loads layer1_squad.jsonl + layer2_reasoning.jsonl (positive examples)
2. Sends each to Claude Haiku to make conversational
3. Saves to `train_conversational_positive.jsonl`
4. Loads layer5_negatives.jsonl (negative examples)
5. Sends each to Claude Haiku to make politely conversational
6. Saves to `train_conversational_negative.jsonl`

**Cost**: ~$2-5 for 150K examples using Claude Haiku

### Step 2: Update Training Configuration

Edit `02_train.py` line 257-269 to use the 3-phase structure above.

### Step 3: Train

```bash
python 02_train.py
```

**Training flow:**
- Epoch 1 (Phase 1): Model learns to extract ‚Üí Loss drops quickly
- Epoch 2 (Phase 2): Model learns conversational style ‚Üí Small bump, then drops
- Epoch 3 (Phase 3): Model learns refusals ‚Üí Converges

---

## Final Model Outputs (100% Confidence)

| Your Query | Model Output (After Training) |
|------------|------------------------------|
| "Find my invoice" | "I found your invoice at Documents/Invoice-2024.pdf." |
| "Why is my Mac slow?" | "Your Mac is slow because RAM is at 97% capacity. Chrome is using 3.2GB - try closing some tabs." |
| "Beach photos?" | "You have beach photos at Photos/IMG_001.jpg (Malibu sunset) and Photos/IMG_002.jpg (waves)." |
| "Who is my boss?" (not in context) | "I couldn't find information about your boss in the provided context." |
| "When is Sarah's birthday?" | "Sarah's birthday is March 15th." |

**Characteristics:**
- Length: 8-30 words (1-2 sentences)
- Style: Natural, helpful, direct
- No preambles like "Based on the context..."
- No hallucinations (foundation phase prevents this)
- Polite refusals when answer not found

---

## Summary

**Your question**: "Are we only adding conversation to phase 2-4?"

**Answer**: No. The recommended approach is:

1. **Phase 1**: Extractive (NOT conversational) - Foundation
2. **Phase 2**: Conversational (YES) - Positive answers
3. **Phase 3**: Conversational (YES) - Refusals

So conversational style is added in Phase 2 & 3 (after extractive foundation in Phase 1).

**Never mix** extractive and conversational in a random order - it confuses the model.
